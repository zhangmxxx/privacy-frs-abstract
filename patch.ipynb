{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from torchvision import transforms, datasets\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "from torch import autograd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_attack import place_mask, postprocess\n",
    "from load_data import MyPatchApplier, TVCalculator, NPSCalculator\n",
    "from facesystem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(image_size=160, margin=0, keep_all=False, min_face_size=40, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_img = []\n",
    "total = 17 # set total to the num of user imgs\n",
    "base_dir = 'datasets/LFW_add_zmx/Zhang_Mingxuan/Zhang_Mingxuan_00' # change me\n",
    "suffix = '.jpg'\n",
    "for i in range(1, total + 1):\n",
    "    if i < 10:\n",
    "        path = base_dir + '0' + str(i) + suffix\n",
    "    else:\n",
    "        path = base_dir + str(i) + suffix\n",
    "    user_img.append(Image.open(path))\n",
    "target_img = Image.open('datasets/LFW_add_zmx/Yves_Brodeur/Yves_Brodeur_0001.jpg') # change target if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map from [0, 1] tensor to [0, 255] IMG\n",
    "# if get [-1, 1] tensor, then wrap\n",
    "PIL    = transforms.ToPILImage()\n",
    "# map from [0, 255] IMG to [0, 1] tensor \n",
    "Tensor = transforms.ToTensor()\n",
    "# map from [0, 1] to [-1, 1]\n",
    "Norm   = transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "# map from [-1, 1] to [0, 1]\n",
    "deNorm = transforms.Normalize((-1, -1, -1), (2, 2, 2))\n",
    "# min-max norm [-1, 1]\n",
    "def m2Norm(patch):\n",
    "    min = torch.min(patch)\n",
    "    max = torch.max(patch)\n",
    "    range = max - min\n",
    "    # [0, 1]\n",
    "    norm_patch = (patch - min) / range\n",
    "    return norm_patch * 2 - 1\n",
    "def norm_loss(patch):\n",
    "    lower = torch.full(patch.shape, -1, device='cuda:0')\n",
    "    upper = torch.full(patch.shape, 1, device='cuda:0')\n",
    "    coeff = 0.01\n",
    "    return coeff * ((patch < lower).sum() + (patch > upper).sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_jitter = transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_mask(num_epochs, lr, net, patch_size):\n",
    "    size = [3, patch_size[0], patch_size[1]]\n",
    "    patch = torch.rand(size, dtype=torch.float, device='cuda:0', requires_grad=True)\n",
    "    # patch = Image.open('12.3/epoch_30.jpg')\n",
    "    # patch = Tensor(patch).cuda()\n",
    "    # patch.requires_grad_()\n",
    "    optimizer = torch.optim.Adam([patch], lr, amsgrad=True)\n",
    "    patch = patch.unsqueeze(0)\n",
    "    scheduler_instantiator = (lambda x: torch.optim.lr_scheduler.ReduceLROnPlateau(x, 'min', patience=50))\n",
    "    scheduler = scheduler_instantiator(optimizer)\n",
    "    PA = MyPatchApplier().cuda()\n",
    "    TV_LOSS = TVCalculator().cuda()\n",
    "    NPS_LOSS = NPSCalculator('non_printability/30values.txt', patch_size).cuda()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # perform norm on the patch      \n",
    "        \n",
    "        nps_loss = NPS_LOSS(patch)\n",
    "        tv_loss = TV_LOSS(patch)\n",
    "        nm_loss = norm_loss(patch)\n",
    "        dist_loss = 0\n",
    "        \n",
    "        target_cropped = mtcnn(target_img).unsqueeze(0).cuda()\n",
    "        target_emb = net(target_cropped)\n",
    "        user_cropped_list = []\n",
    "        positions = []\n",
    "        for i in range(total):\n",
    "            # place_mask return uint8\n",
    "            position = PA.get_patch_position(user_img[i])\n",
    "            positions.append(position)\n",
    "            mimg = place_mask(user_img[i], 'surgical')\n",
    "            mimg = cv2.cvtColor(np.asarray(mimg), cv2.COLOR_BGR2RGB)\n",
    "            mimg = Image.fromarray(mimg)\n",
    "            user_cropped_list.append(mtcnn(mimg))\n",
    "        cropped_batch = torch.stack(user_cropped_list).cuda()\n",
    "        # positions = torch.stack(position_list).cuda()\n",
    "        # cropped_batch = color_jitter(cropped_batch)\n",
    "        patch_batch = patch.expand(total, 3, patch_size[0], patch_size[1]).cuda()\n",
    "        img_patch_batch = PA.apply_with_given_pos(cropped_batch, patch_batch, positions)\n",
    "        user_emb_batch = net(img_patch_batch)\n",
    "        target_emb_batch = target_emb.expand(total, 512)\n",
    "\n",
    "        t = torch.ones(total).cuda()\n",
    "        f = F.cosine_similarity(user_emb_batch, target_emb_batch)\n",
    "        print(f)\n",
    "        dist = t - f\n",
    "        dist_loss = torch.mean(dist)\n",
    "        main_loss = nps_loss + tv_loss + dist_loss\n",
    "        # if last epoch, keep the normed patch\n",
    "        if (epoch != num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            main_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(main_loss)\n",
    "        print(\"finish epoch\", epoch)\n",
    "        print(\"similarity: \", 1 - dist_loss)\n",
    "        # visualize, but do not use it for recognition\n",
    "        if (epoch % 10 == 0):\n",
    "            img = patch.clamp(-1, 1)\n",
    "            img = deNorm(img.squeeze())\n",
    "            img = PIL(img)\n",
    "            if not os.path.exists('./run_result'):\n",
    "                os.makedirs('./run_result')\n",
    "            img.save(f'./run_result/epoch_{epoch}.jpg')\n",
    "    patch = patch.clamp(-1, 1) # [-1, 1]\n",
    "    torch.save(patch, 'patch.pt')\n",
    "    patch = deNorm(patch.squeeze()) # [0, 1]\n",
    "    patch = PIL(patch)\n",
    "    patch.save('patch.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_without_mask(num_epochs, lr, net, patch_size):\n",
    "    size = [3, patch_size[0], patch_size[1]]\n",
    "    patch = torch.rand(size, dtype=torch.float, device='cuda:0', requires_grad=True)\n",
    "    # patch = Image.open('12.3/epoch_30.jpg')\n",
    "    # patch = Tensor(patch).cuda()\n",
    "    # patch.requires_grad_()\n",
    "    optimizer = torch.optim.Adam([patch], lr, amsgrad=True)\n",
    "    patch = patch.unsqueeze(0)\n",
    "    scheduler_instantiator = (lambda x: torch.optim.lr_scheduler.ReduceLROnPlateau(x, 'min', patience=50))\n",
    "    scheduler = scheduler_instantiator(optimizer)\n",
    "    patch_applier = MyPatchApplier().cuda()\n",
    "    TV_LOSS = TVCalculator().cuda()\n",
    "    NPS_LOSS = NPSCalculator('non_printability/30values.txt', patch_size).cuda()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # perform norm on the patch      \n",
    "        \n",
    "        nps_loss = NPS_LOSS(patch)\n",
    "        tv_loss = TV_LOSS(patch)\n",
    "        nm_loss = norm_loss(patch)\n",
    "        dist_loss = 0\n",
    "        \n",
    "        target_cropped = mtcnn(target_img).unsqueeze(0).cuda()\n",
    "        target_emb = net(target_cropped)\n",
    "        # 不支持batch-mtcnn(img不能stack)\n",
    "        user_cropped_list = []\n",
    "        for i in range(total):\n",
    "            user_cropped_list.append(mtcnn(user_img[i]))\n",
    "        cropped_batch = torch.stack(user_cropped_list).cuda()\n",
    "        # cropped_batch = color_jitter(cropped_batch)\n",
    "        patch_batch = patch.expand(total, 3, patch_size[0], patch_size[1]).cuda()\n",
    "        img_patch_batch = patch_applier(cropped_batch, patch_batch)\n",
    "        user_emb_batch = net(img_patch_batch)\n",
    "        target_emb_batch = target_emb.expand(total, 512)\n",
    "\n",
    "        t = torch.ones(total).cuda()\n",
    "        f = F.cosine_similarity(user_emb_batch, target_emb_batch)\n",
    "        print(f)\n",
    "        dist = t - f\n",
    "        dist_loss = torch.mean(dist)\n",
    "        main_loss = nps_loss + tv_loss + dist_loss\n",
    "        # if last epoch, keep the normed patch\n",
    "        if (epoch != num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            main_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(main_loss)\n",
    "        print(\"finish epoch\", epoch)\n",
    "        print(\"similarity: \", 1 - dist_loss)\n",
    "        # visualize, but do not use it for recognition\n",
    "        if (epoch % 10 == 0):\n",
    "            img = patch.clamp(-1, 1)\n",
    "            img = deNorm(img.squeeze())\n",
    "            img = PIL(img)\n",
    "            if not os.path.exists('./run_result'):\n",
    "                os.makedirs('./run_result')\n",
    "            img.save(f'./run_result/epoch_{epoch}.jpg')\n",
    "    patch = patch.clamp(-1, 1) # [-1, 1]\n",
    "    torch.save(patch, 'patch.pt')\n",
    "    patch = deNorm(patch.squeeze()) # [0, 1]\n",
    "    patch = PIL(patch)\n",
    "    patch.save('patch.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = InceptionResnetV1(pretrained='vggface2', device='cuda:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_mask(300, 0.01, net, (80, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = torch.load('./patch.pt')\n",
    "\n",
    "fs = FaceSystem()\n",
    "patch_applier = MyPatchApplier().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dist = 0\n",
    "for i in range(total):\n",
    "    # _user_img = postprocess(loader(user_img[i]).unsqueeze(0))\n",
    "    user_img_cropped = mtcnn(user_img[i])\n",
    "    user_with_patch = patch_applier(user_img_cropped.unsqueeze(0), patch)\n",
    "    user_ori_emb = net(user_with_patch.cuda())\n",
    "    name, min_dist, target_dist = fs.get_id_ori_result(user_ori_emb)\n",
    "    test_dist += target_dist\n",
    "    print(name, min_dist, target_dist)\n",
    "print(test_dist / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
